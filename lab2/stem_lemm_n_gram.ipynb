{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9b85a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'amazing'), ('amazing', 'and'), ('and', 'natural'), ('natural', 'language'), ('language', 'is'), ('is', 'powerful')]\n",
      "Bigram counts: Counter({('natural', 'language'): 2, ('language', 'processing'): 1, ('processing', 'is'): 1, ('is', 'amazing'): 1, ('amazing', 'and'): 1, ('and', 'natural'): 1, ('language', 'is'): 1, ('is', 'powerful'): 1})\n",
      "\n",
      "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'amazing'), ('is', 'amazing', 'and'), ('amazing', 'and', 'natural'), ('and', 'natural', 'language'), ('natural', 'language', 'is'), ('language', 'is', 'powerful')]\n",
      "\n",
      "4-grams: [('natural', 'language', 'processing', 'is'), ('language', 'processing', 'is', 'amazing'), ('processing', 'is', 'amazing', 'and'), ('is', 'amazing', 'and', 'natural'), ('amazing', 'and', 'natural', 'language'), ('and', 'natural', 'language', 'is'), ('natural', 'language', 'is', 'powerful')]\n",
      "\n",
      "Character bigrams: ['py', 'yt', 'th', 'ho', 'on']\n"
     ]
    }
   ],
   "source": [
    "# N-GRAMS EXAMPLES\n",
    "from collections import Counter\n",
    "\n",
    "text = \"natural language processing is amazing and natural language is powerful\"\n",
    "words = text.split()\n",
    "\n",
    "# Bigrams (2-grams)\n",
    "bigrams = [tuple(words[i:i+2]) for i in range(len(words)-1)]\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Bigram counts:\", Counter(bigrams))\n",
    "\n",
    "# Trigrams (3-grams)\n",
    "trigrams = [tuple(words[i:i+3]) for i in range(len(words)-2)]\n",
    "print(\"\\nTrigrams:\", trigrams)\n",
    "\n",
    "# General n-gram function\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "print(\"\\n4-grams:\", generate_ngrams(text, 4))\n",
    "\n",
    "# Character-level n-grams\n",
    "word = \"python\"\n",
    "char_bigrams = [word[i:i+2] for i in range(len(word)-1)]\n",
    "print(\"\\nCharacter bigrams:\", char_bigrams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b201194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Porter Stemmer ---\n",
      "running -> run\n",
      "runs -> run\n",
      "ran -> ran\n",
      "runner -> runner\n",
      "easily -> easili\n",
      "fairly -> fairli\n",
      "playing -> play\n",
      "played -> play\n",
      "\n",
      "--- Snowball Stemmer ---\n",
      "running -> run\n",
      "runs -> run\n",
      "ran -> ran\n",
      "runner -> runner\n",
      "easily -> easili\n",
      "fairly -> fair\n",
      "playing -> play\n",
      "played -> play\n",
      "\n",
      "--- Lancaster Stemmer ---\n",
      "running -> run\n",
      "runs -> run\n",
      "ran -> ran\n",
      "runner -> run\n",
      "easily -> easy\n",
      "fairly -> fair\n",
      "playing -> play\n",
      "played -> play\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dimtriospanagoulias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# STEMMING EXAMPLES\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Porter Stemmer (most common)\n",
    "porter = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\", \"playing\", \"played\"]\n",
    "print(\"\\n--- Porter Stemmer ---\")\n",
    "for word in words:\n",
    "    print(f\"{word} -> {porter.stem(word)}\")\n",
    "\n",
    "# Snowball Stemmer (supports multiple languages)\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "print(\"\\n--- Snowball Stemmer ---\")\n",
    "for word in words:\n",
    "    print(f\"{word} -> {snowball.stem(word)}\")\n",
    "\n",
    "# Lancaster Stemmer (most aggressive)\n",
    "lancaster = LancasterStemmer()\n",
    "print(\"\\n--- Lancaster Stemmer ---\")\n",
    "for word in words:\n",
    "    print(f\"{word} -> {lancaster.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efcb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lemmatization (default = noun) ---\n",
      "running -> running\n",
      "runs -> run\n",
      "ran -> ran\n",
      "runner -> runner\n",
      "better -> better\n",
      "worse -> worse\n",
      "feet -> foot\n",
      "geese -> goose\n",
      "\n",
      "--- Lemmatization with POS tags ---\n",
      "running (verb) -> run\n",
      "running (noun) -> running\n",
      "better (adjective) -> good\n",
      "worse (adjective) -> bad\n",
      "is (verb) -> be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dimtriospanagoulias/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/dimtriospanagoulias/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION EXAMPLES\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\n--- Lemmatization (default = noun) ---\")\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\", \"better\", \"worse\", \"feet\", \"geese\"]\n",
    "for word in words:\n",
    "    print(f\"{word} -> {lemmatizer.lemmatize(word)}\")\n",
    "\n",
    "# Lemmatization with POS tags\n",
    "print(\"\\n--- Lemmatization with POS tags ---\")\n",
    "print(f\"running (verb) -> {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"running (noun) -> {lemmatizer.lemmatize('running', pos='n')}\")\n",
    "print(f\"better (adjective) -> {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "print(f\"worse (adjective) -> {lemmatizer.lemmatize('worse', pos='a')}\")\n",
    "print(f\"is (verb) -> {lemmatizer.lemmatize('is', pos='v')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77524f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Stemming vs Lemmatization Comparison ---\n",
      "studies      | Stem: studi      | Lemma(n): study      | Lemma(v): study\n",
      "studying     | Stem: studi      | Lemma(n): studying   | Lemma(v): study\n",
      "better       | Stem: better     | Lemma(n): better     | Lemma(v): better\n",
      "worse        | Stem: wors       | Lemma(n): worse      | Lemma(v): worse\n",
      "caring       | Stem: care       | Lemma(n): caring     | Lemma(v): care\n",
      "cared        | Stem: care       | Lemma(n): cared      | Lemma(v): care\n",
      "cats         | Stem: cat        | Lemma(n): cat        | Lemma(v): cat\n",
      "geese        | Stem: gees       | Lemma(n): goose      | Lemma(v): geese\n"
     ]
    }
   ],
   "source": [
    "# COMPARISON: Stemming vs Lemmatization\n",
    "print(\"\\n--- Stemming vs Lemmatization Comparison ---\")\n",
    "test_words = [\"studies\", \"studying\", \"better\", \"worse\", \"caring\", \"cared\", \"cats\", \"geese\"]\n",
    "for word in test_words:\n",
    "    stemmed = porter.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    lemmatized_verb = lemmatizer.lemmatize(word, pos='v')\n",
    "    print(f\"{word:12} | Stem: {stemmed:10} | Lemma(n): {lemmatized:10} | Lemma(v): {lemmatized_verb}\")\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpunipi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"I love NLP and Machine Learning.\",\n",
    "    \"NLP is amazing and powerful!\",\n",
    "    \"Machine Learning is a subset of AI.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ai.', 'amazing', 'and', 'i', 'is', 'learning', 'learning.', 'love', 'machine', 'nlp', 'of', 'powerful!', 'subset']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>ai.</th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>learning.</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>nlp</th>\n",
       "      <th>of</th>\n",
       "      <th>powerful!</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  ai.  amazing  and  i  is  learning  learning.  love  machine  nlp  of  \\\n",
       "0  0    0        0    1  1   0         0          1     1        1    1   0   \n",
       "1  0    0        1    1  0   1         0          0     0        0    1   0   \n",
       "2  1    1        0    0  0   1         1          0     0        1    0   1   \n",
       "\n",
       "   powerful!  subset  \n",
       "0          0       0  \n",
       "1          1       0  \n",
       "2          0       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of words\n",
    "#Tokenize, voc \n",
    "vocab = set(word.lower() for doc in documents for word in doc.split())\n",
    "vocab = sorted(vocab)\n",
    "print(vocab)\n",
    "# On documents we create a BoW\n",
    "bow_vec = []\n",
    "for doc in documents:\n",
    "    words = doc.lower().split()\n",
    "    bow_vec.append([words.count(word) for word in vocab])\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(bow_vec, columns=vocab)\n",
    "df\n",
    "\n",
    "#manual \n",
    "\n",
    "# Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'i': 1, 'love': 1, 'nlp': 1, 'and': 1, 'machine': 1, 'learning': 1, '.': 1}), Counter({'nlp': 1, 'is': 1, 'amazing': 1, 'and': 1, 'powerful': 1, '!': 1}), Counter({'machine': 1, 'learning': 1, 'is': 1, 'a': 1, 'subset': 1, 'of': 1, 'ai': 1, '.': 1})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dimtriospanagoulias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <th>a</th>\n",
       "      <th>ai</th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>nlp</th>\n",
       "      <th>of</th>\n",
       "      <th>powerful</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  .  a  ai  amazing  and  i  is  learning  love  machine  nlp  of  \\\n",
       "0  0  1  0   0        0    1  1   0         1     1        1    1   0   \n",
       "1  1  0  0   0        1    1  0   1         0     0        0    1   0   \n",
       "2  0  1  1   1        0    0  0   1         1     0        1    0   1   \n",
       "\n",
       "   powerful  subset  \n",
       "0         0       0  \n",
       "1         1       0  \n",
       "2         0       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk \n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "bow_vec2 = [Counter(word_tokenize(doc.lower())) for doc in documents]\n",
    "print(bow_vec2)\n",
    "#get unique words \n",
    "vocab2 = sorted(set(word for doc in bow_vec2 for word in doc))\n",
    "\n",
    "#matrix \n",
    "bow_mat = [[bow[word] if word in bow else 0 for word in vocab2] for bow in bow_vec2]\n",
    "df = pd.DataFrame(bow_mat, columns=vocab2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>ai.</th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>learning.</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>nlp</th>\n",
       "      <th>of</th>\n",
       "      <th>powerful!</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214614</td>\n",
       "      <td>0.282191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282191</td>\n",
       "      <td>0.282191</td>\n",
       "      <td>0.214614</td>\n",
       "      <td>0.214614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338629</td>\n",
       "      <td>0.257536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.241878</td>\n",
       "      <td>0.241878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183955</td>\n",
       "      <td>0.241878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a       ai.   amazing       and         i        is  learning  \\\n",
       "0  0.000000  0.000000  0.000000  0.214614  0.282191  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.338629  0.257536  0.000000  0.257536  0.000000   \n",
       "2  0.241878  0.241878  0.000000  0.000000  0.000000  0.183955  0.241878   \n",
       "\n",
       "   learning.      love   machine       nlp        of  powerful!    subset  \n",
       "0   0.282191  0.282191  0.214614  0.214614  0.000000   0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.257536  0.000000   0.338629  0.000000  \n",
       "2   0.000000  0.000000  0.183955  0.000000  0.241878   0.000000  0.241878  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-idf \n",
    "#manual \n",
    "import numpy as np\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "# print(tokenized_docs)\n",
    "vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
    "# print(vocab)\n",
    "# matrix\n",
    "tfi_matrix = []\n",
    "for doc in tokenized_docs:\n",
    "    word_counts = Counter(doc)\n",
    "    tf_vector = [word_counts[word] / len(doc) for word in vocab]\n",
    "    tfi_matrix.append(tf_vector)\n",
    "# print(tfidf_matrix)\n",
    "df_vector = [sum(1 for doc in tokenized_docs if word in doc) for word in vocab]\n",
    "#Computing inverse document frequency (IDF)\n",
    "N = len(documents)\n",
    "idf_vector = [np.log((N+1)/(df+1))+1 for df in df_vector]\n",
    "tf_idfFinalMatrix = np.array(tfi_matrix) * np.array(idf_vector)\n",
    "#auto \n",
    "tf_idfFinalMatrix\n",
    "\n",
    "#Convert to DataFrame\n",
    "df = pd.DataFrame(tf_idfFinalMatrix, columns=vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>nlp</th>\n",
       "      <th>of</th>\n",
       "      <th>powerful</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.417796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417796</td>\n",
       "      <td>0.549351</td>\n",
       "      <td>0.417796</td>\n",
       "      <td>0.417796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.459548</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459548</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.459548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai  amazing       and        is  learning      love   machine  \\\n",
       "0  0.000000  0.00000  0.417796  0.000000  0.417796  0.549351  0.417796   \n",
       "1  0.000000  0.51742  0.393511  0.393511  0.000000  0.000000  0.000000   \n",
       "2  0.459548  0.00000  0.000000  0.349498  0.349498  0.000000  0.349498   \n",
       "\n",
       "        nlp        of  powerful    subset  \n",
       "0  0.417796  0.000000   0.00000  0.000000  \n",
       "1  0.393511  0.000000   0.51742  0.000000  \n",
       "2  0.000000  0.459548   0.00000  0.459548  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Auto feature extraction of Tf-idf features in docs\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_fullauto_matrix = vectorizer.fit_transform(documents)\n",
    "# Convert to DataFrame\n",
    "df_idf_auto = pd.DataFrame(tfidf_fullauto_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_idf_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings random mockup \n",
    "text=\"the cat sat on the mat. the dog ran after the cat. a cat is a small animal. the dog is a loyal animal. cats and dogs are pets. the mat is on the floor. animals are living creatures. pets need care and love. the cat sleeps on the mat. dogs like to run and play.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat sat on the mat. the dog ran after the cat. a cat is a small animal. the dog is a loyal animal. cats and dogs are pets. the mat is on the floor. animals are living creatures. pets need care and love. the cat sleeps on the mat. dogs like to run and play.\n",
      "{'need': 0, 'play.': 1, 'dog': 2, 'on': 3, 'floor.': 4, 'are': 5, 'loyal': 6, 'living': 7, 'pets': 8, 'run': 9, 'care': 10, 'dogs': 11, 'cat.': 12, 'small': 13, 'animal.': 14, 'to': 15, 'creatures.': 16, 'like': 17, 'sleeps': 18, 'a': 19, 'mat': 20, 'after': 21, 'ran': 22, 'mat.': 23, 'is': 24, 'and': 25, 'love.': 26, 'the': 27, 'cats': 28, 'sat': 29, 'cat': 30, 'animals': 31, 'pets.': 32}\n",
      "{0: 'need', 1: 'play.', 2: 'dog', 3: 'on', 4: 'floor.', 5: 'are', 6: 'loyal', 7: 'living', 8: 'pets', 9: 'run', 10: 'care', 11: 'dogs', 12: 'cat.', 13: 'small', 14: 'animal.', 15: 'to', 16: 'creatures.', 17: 'like', 18: 'sleeps', 19: 'a', 20: 'mat', 21: 'after', 22: 'ran', 23: 'mat.', 24: 'is', 25: 'and', 26: 'love.', 27: 'the', 28: 'cats', 29: 'sat', 30: 'cat', 31: 'animals', 32: 'pets.'}\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "print(text)\n",
    "\n",
    "words = text.split()\n",
    "#creating the vocabulary \n",
    "word_to_ix = {word: i for i, word in enumerate(set(words))}\n",
    "ix_to_word = {i: word for i, word in enumerate(set(words))}\n",
    "vocab_size = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "print(ix_to_word)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the skipgram model\n",
    "def create_skip_grams(words, window_size=2):\n",
    "    skip_grams = []\n",
    "    for i, word in enumerate(words):\n",
    "        context_words = words[max(0, i - window_size): i] + words[i + 1: i + window_size + 1]\n",
    "        for context_word in context_words:\n",
    "            skip_grams.append((word, context_word))\n",
    "    return skip_grams\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        output = self.output(embeds)\n",
    "        return output\n",
    "    \n",
    "# Model parameters to optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 0, Loss: 788.1608464717865\n",
      "Epoch 10, Loss: 646.25643658638\n",
      "Epoch 20, Loss: 604.7178028821945\n",
      "Epoch 30, Loss: 576.2762328386307\n",
      "Epoch 40, Loss: 552.7872568964958\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10  \n",
    "print(embedding_dim)\n",
    "\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  \n",
    "\n",
    "# Training \n",
    "pairs = create_skip_grams(words)\n",
    "n_epochs= 50    \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss  = 0 \n",
    "    for word, context in pairs:\n",
    "        model.zero_grad()\n",
    "        word_idx = torch.tensor([word_to_ix[word]], dtype=torch.long)\n",
    "        context_idx = torch.tensor([word_to_ix[context]], dtype=torch.long)\n",
    "        \n",
    "        output = model(word_idx)\n",
    "        loss = criterion(output, context_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n",
    "\n",
    "    #Get word vectors \n",
    "    word_vectors = {}\n",
    "    for word in word_to_ix:\n",
    "        word_idx = torch.tensor([word_to_ix[word]], dtype=torch.long)\n",
    "        word_vectors[word] = model.embeddings(word_idx).detach().numpy()\n",
    "\n",
    "    # Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos similarity with word cat\n",
      "Word: loyal, Similarity: 0.4762212038040161\n",
      "Word: living, Similarity: 0.4138075113296509\n",
      "Word: like, Similarity: 0.362325519323349\n",
      "Word: and, Similarity: 0.3419911861419678\n",
      "Word: mat, Similarity: 0.27234262228012085\n",
      "Word: creatures., Similarity: 0.2681143581867218\n",
      "Word: care, Similarity: 0.2499457597732544\n",
      "Word: the, Similarity: 0.23242531716823578\n",
      "Word: on, Similarity: 0.2235090285539627\n",
      "Word: animal., Similarity: 0.19857393205165863\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "print(\"cos similarity with word cat\")\n",
    "cat_vectro = word_vectors[\"cat\"].flatten()\n",
    "similarities=[]\n",
    "for word, vector in word_vectors.items():\n",
    "    if word != \"cat\":\n",
    "        word_vector = word_vectors[word].flatten()\n",
    "        sim = float(cosine_similarity(cat_vectro, word_vector))\n",
    "        similarities.append((word, sim))\n",
    "\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "for word, sim in similarities[:10]:\n",
    "    print(f\"Word: {word}, Similarity: {sim}\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "1. Import Required Libraries\n",
    "2. Preprocessing: Tokenization and Subword Generation\n",
    "3. Building the Vocabulary and Subword Index\n",
    "4. Creating the Skip-Gram Training Data\n",
    "5. Initializing Model Parameters\n",
    "6. Training FastText with Negative Sampling\n",
    "7. Using the Trained Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpunipi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

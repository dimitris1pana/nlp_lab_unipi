{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**softmax**\n",
    "\n",
    "- $\\sigma$\t=\tsoftmax\n",
    "- $\\vec{z}$\t=\tinput vector\n",
    "- $e^{z_{i}}$\t=\tstandard exponential function for input vector\n",
    "- $K$\t=\tnumber of classes in the multi-class classifier\n",
    "- $e^{z_{j}}$\t=\tstandard exponential function for output vector\n",
    "- $e^{z_{j}}$\t=\tstandard exponential function for output vector\n",
    "\n",
    "------------\n",
    "![image](/Users/dimtriospanagoulias/Downloads/NLP_UNIPI/nlp_lab/Neuron2transformer/img_blog_image1_inline_(2).webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Proof:\n",
    "\n",
    "- Let's call the max value c\n",
    "    - The modified formula is: $\\frac{e^{x_i-c}}{\\sum_{j=1}^n e^{x_j-c}}$\n",
    "    - This simplifies to: $\\frac{e^{x_i}/e^c}{\\sum_{j=1}^n e^{x_j}/e^c} = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$\n",
    "\n",
    "- - Why Use This Form:\n",
    "\n",
    "    - Prevents numerical overflow\n",
    "    - Avoids inf values when dealing with large numbers\n",
    "    - More stable training in deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# Softmax function for attention mechanism \n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled dot-product attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)  # (seq_len, seq_len)\n",
    "    weights = softmax(scores)\n",
    "    return np.matmul(weights, V), weights  # Output and attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def positional_encoding(seq_len,d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 /np.power(10000, (2*(i//2)) / d_model)\n",
    "    angles = pos * angle_rates\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple layer normalization\n",
    "def layer_norm(X, eps=1e-6):\n",
    "    mean = X.mean( axis=-1, keepdims=True)\n",
    "    std = X.std( axis=-1, keepdims=True)\n",
    "    return (X - mean) / (std + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split heads for multi-head attention\n",
    "def split_heads(X, num_heads):\n",
    "    seq_len , d_model = X.shape\n",
    "    depth = d_model // num_heads\n",
    "    X = X.reshape(seq_len, num_heads, depth)\n",
    "    return np.transpose(X,[1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def multi_head_attention(X, num_heads=2):\n",
    "    seq_len, d_model = X.shape\n",
    "    assert d_model % num_heads == 0\n",
    "    W_Q = np.random.randn(d_model, d_model)\n",
    "    W_K = np.random.randn(d_model, d_model)\n",
    "    W_V = np.random.randn(d_model, d_model)\n",
    "    W_O = np.random.randn(d_model, d_model)\n",
    "\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "\n",
    "    Q_heads = split_heads(Q, num_heads)\n",
    "    K_heads = split_heads(K, num_heads)\n",
    "    V_heads = split_heads(V, num_heads)\n",
    "\n",
    "    head_outputs = []\n",
    "    attn_weights = []\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        out, weights = scaled_dot_product_attention(Q_heads[i], K_heads[i], V_heads[i])\n",
    "        head_outputs.append(out)\n",
    "        attn_weights.append(weights)\n",
    "\n",
    "    concat = np.transpose(np.array(head_outputs), (1, 0, 2)).reshape(seq_len, d_model)\n",
    "    output = concat @ W_O\n",
    "    print(\"WQ\",pd.DataFrame(W_Q).head())\n",
    "    print(\"WK\",pd.DataFrame(W_K).head())\n",
    "    print(\"WV\",pd.DataFrame(W_V).head())\n",
    "    print(\"WO\",pd.DataFrame(W_O).head())\n",
    "    print(\"Q\",pd.DataFrame(Q).head())\n",
    "    print(\"K\",pd.DataFrame(K).head())\n",
    "    print(\"V\",pd.DataFrame(V).head())\n",
    "    print(\"Attention Weights for each head:\")\n",
    "    for i, weights in enumerate(attn_weights):\n",
    "        print(f\"weights {i+1}:\\n\", pd.DataFrame(weights).head())\n",
    "    for i, head_outputs in enumerate(head_outputs):\n",
    "        print(f\"Head {i+1}:\\n\", pd.DataFrame(head_outputs).head())\n",
    "    \n",
    "    return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X):\n",
    "    W1= np.random.rand( X.shape[1],4)\n",
    "    W2 = np.random.rand(4, X.shape[1])\n",
    "    return np.dot(np.maximum(0,np.dot(X,W1)),W2) # ReLU activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(X, num_heads=2):\n",
    "    # Positional Encoding\n",
    "    pe = positional_encoding(X.shape[0], X.shape[1])\n",
    "    X_pos = X + pe\n",
    "\n",
    "    # Multi-head attention with residual connection and layer norm\n",
    "    attn_output, attn_weights = multi_head_attention(X_pos, num_heads)\n",
    "    attn_output = layer_norm(X_pos + attn_output)\n",
    "\n",
    "    # Feed-forward network with residual and layer norm\n",
    "    ff_output = feed_forward(attn_output)\n",
    "    final_output = layer_norm(attn_output + ff_output)\n",
    "\n",
    "    return final_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input (for 4 words, each with 8 features)\n",
    "np.random.seed(0) # for reproducibility\n",
    "X = np.random.rand(4, 8) # 4 words, each with 8 features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ           0         1         2         3         4         5         6  \\\n",
      "0  2.269755 -1.454366  0.045759 -0.187184  1.532779  1.469359  0.154947   \n",
      "1 -0.887786 -1.980796 -0.347912  0.156349  1.230291  1.202380 -0.387327   \n",
      "2 -1.048553 -1.420018 -1.706270  1.950775 -0.509652 -0.438074 -1.252795   \n",
      "3 -1.613898 -0.212740 -0.895467  0.386902 -0.510805 -1.180632 -0.028182   \n",
      "4  0.066517  0.302472 -0.634322 -0.362741 -0.672460 -0.359553 -0.813146   \n",
      "\n",
      "          7  \n",
      "0  0.378163  \n",
      "1 -0.302303  \n",
      "2  0.777490  \n",
      "3  0.428332  \n",
      "4 -1.726283  \n",
      "WK           0         1         2         3         4         5         6  \\\n",
      "0 -1.070753  1.054452 -0.403177  1.222445  0.208275  0.976639  0.356366   \n",
      "1  0.010500  1.785870  0.126912  0.401989  1.883151 -1.347759 -1.270485   \n",
      "2 -1.173123  1.943621 -0.413619 -0.747455  1.922942  1.480515  1.867559   \n",
      "3 -0.861226  1.910065 -0.268003  0.802456  0.947252 -0.155010  0.614079   \n",
      "4  0.376426 -1.099401  0.298238  1.326386 -0.694568 -0.149635 -0.435154   \n",
      "\n",
      "          7  \n",
      "0  0.706573  \n",
      "1  0.969397  \n",
      "2  0.906045  \n",
      "3  0.922207  \n",
      "4  1.849264  \n",
      "WV           0         1         2         3         4         5         6  \\\n",
      "0 -0.744755 -0.826439 -0.098453 -0.663478  1.126636 -1.079932 -1.147469   \n",
      "1 -0.498032  1.929532  0.949421  0.087551 -1.225436  0.844363 -1.000215   \n",
      "2  1.188030  0.316943  0.920859  0.318728  0.856831 -0.651026 -1.034243   \n",
      "3 -0.803410 -0.689550 -0.455533  0.017479 -0.353994 -1.374951 -0.643618   \n",
      "4  0.625231 -1.602058 -1.104383  0.052165 -0.739563  1.543015 -1.292857   \n",
      "\n",
      "          7  \n",
      "0 -0.437820  \n",
      "1 -1.544771  \n",
      "2  0.681595  \n",
      "3 -2.223403  \n",
      "4  0.267051  \n",
      "WO           0         1         2         3         4         5         6  \\\n",
      "0 -0.017020  0.379152  2.259309 -0.042257 -0.955945 -0.345982 -0.463596   \n",
      "1 -1.540797  0.063262  0.156507  0.232181 -0.597316 -0.237922 -1.424061   \n",
      "2 -0.542861  0.416050 -1.156182  0.781198  1.494485 -2.069985  0.426259   \n",
      "3 -0.637437 -0.397272 -0.132881 -0.297791 -0.309013 -1.676004  1.152332   \n",
      "4 -0.813364 -1.466424  0.521065 -0.575788  0.141953 -0.319328  0.691539   \n",
      "\n",
      "          7  \n",
      "0  0.481481  \n",
      "1 -0.493320  \n",
      "2  0.676908  \n",
      "3  1.079619  \n",
      "4  0.694749  \n",
      "Q           0         1         2         3         4         5         6  \\\n",
      "0 -4.787769 -4.749540 -4.878371 -0.658861  2.511235  4.047184  1.571185   \n",
      "1 -0.921637 -5.753740 -6.095093  1.097368  2.089980  3.484910  0.954928   \n",
      "2 -3.430943 -3.346605 -5.976278 -0.069939  0.436808  2.096155  1.160845   \n",
      "3 -4.101811  0.155352 -4.038217 -0.966605 -0.473992  0.608670  2.162846   \n",
      "\n",
      "          7  \n",
      "0 -0.015456  \n",
      "1  0.808235  \n",
      "2 -0.008662  \n",
      "3  0.284544  \n",
      "K           0         1         2         3         4         5         6  \\\n",
      "0  3.419435  9.664639 -3.361354  5.233854  1.420646 -1.814686 -1.196220   \n",
      "1 -0.137035  9.357562 -3.661673  6.075135  2.035509  1.106372  0.532449   \n",
      "2  2.365646  8.376503 -3.750750  5.835079 -0.332133  0.795540  1.139832   \n",
      "3  3.108401  5.720006 -3.277590  4.556963 -2.032158  0.341431  1.307737   \n",
      "\n",
      "           7  \n",
      "0   9.437232  \n",
      "1   8.663905  \n",
      "2  10.060260  \n",
      "3   7.180653  \n",
      "V           0         1         2         3         4         5         6  \\\n",
      "0 -1.219793 -0.909985  4.022149  0.631755  1.210679 -1.241804 -2.747614   \n",
      "1 -1.406063 -3.776895  2.264023 -0.714098  3.085774 -1.991070 -2.508579   \n",
      "2 -0.344547 -4.917614  2.329408  0.375434  3.065568 -2.389433 -2.776557   \n",
      "3 -0.336577 -4.835104  1.220386  0.494661  2.674432 -2.717896 -0.960634   \n",
      "\n",
      "          7  \n",
      "0 -4.699667  \n",
      "1 -2.768885  \n",
      "2 -2.899114  \n",
      "3 -2.385614  \n",
      "Attention Weights for each head:\n",
      "weights 1:\n",
      "           0         1         2         3\n",
      "0  0.000024  0.388253  0.013419  0.598304\n",
      "1  0.000019  0.000937  0.005721  0.993322\n",
      "2  0.000340  0.603900  0.056054  0.339707\n",
      "3  0.000565  0.991228  0.007285  0.000922\n",
      "weights 2:\n",
      "           0         1         2         3\n",
      "0  0.000306  0.955847  0.041559  0.002288\n",
      "1  0.001732  0.892790  0.102809  0.002669\n",
      "2  0.007812  0.522221  0.317889  0.152078\n",
      "3  0.009384  0.114631  0.429981  0.446004\n",
      "Head 1:\n",
      "           0         1         2         3\n",
      "0 -0.751936 -4.425265  1.640530  0.023760\n",
      "1 -0.337642 -4.834510  1.227763  0.492849\n",
      "2 -0.983186 -4.199344  1.913755 -0.241945\n",
      "3 -1.397239 -3.784561  2.264531 -0.704286\n",
      "Head 2:\n",
      "           0         1         2         3\n",
      "0  3.083420 -2.009060 -2.516247 -2.774011\n",
      "1  3.079352 -2.032668 -2.532412 -2.784594\n",
      "2  3.002147 -2.222387 -2.360224 -2.767079\n",
      "3  2.876029 -2.479494 -1.935658 -2.672060\n"
     ]
    }
   ],
   "source": [
    "encoded_output, attention_weights = encoder_block(X, num_heads=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.322217</td>\n",
       "      <td>-0.703987</td>\n",
       "      <td>-0.625960</td>\n",
       "      <td>-1.116872</td>\n",
       "      <td>0.666319</td>\n",
       "      <td>-0.886659</td>\n",
       "      <td>2.116349</td>\n",
       "      <td>0.228593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.530799</td>\n",
       "      <td>-0.939979</td>\n",
       "      <td>-0.369100</td>\n",
       "      <td>-1.179531</td>\n",
       "      <td>0.499107</td>\n",
       "      <td>-0.845872</td>\n",
       "      <td>2.070712</td>\n",
       "      <td>0.233863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.305734</td>\n",
       "      <td>-0.837191</td>\n",
       "      <td>-0.630886</td>\n",
       "      <td>-1.016384</td>\n",
       "      <td>0.882324</td>\n",
       "      <td>-0.856878</td>\n",
       "      <td>2.062420</td>\n",
       "      <td>0.090862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130583</td>\n",
       "      <td>-0.775043</td>\n",
       "      <td>-0.763410</td>\n",
       "      <td>-0.855956</td>\n",
       "      <td>1.100317</td>\n",
       "      <td>-0.804030</td>\n",
       "      <td>2.050076</td>\n",
       "      <td>-0.082535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.322217 -0.703987 -0.625960 -1.116872  0.666319 -0.886659  2.116349   \n",
       "1  0.530799 -0.939979 -0.369100 -1.179531  0.499107 -0.845872  2.070712   \n",
       "2  0.305734 -0.837191 -0.630886 -1.016384  0.882324 -0.856878  2.062420   \n",
       "3  0.130583 -0.775043 -0.763410 -0.855956  1.100317 -0.804030  2.050076   \n",
       "\n",
       "          7  \n",
       "0  0.228593  \n",
       "1  0.233863  \n",
       "2  0.090862  \n",
       "3 -0.082535  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataframe = pd.DataFrame(encoded_output)\n",
    "dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(X, encoder_output,num_heads=2, mask=None):\n",
    "    seq_len ,d_model = X.shape\n",
    "    #Positional Encoding\n",
    "    pe = positional_encoding(seq_len, d_model)\n",
    "    X_post = X+ pe\n",
    "    #Masked Multi-head attention with residual connection and layer norm\n",
    "    def masked_multi_head_attention(X,mask=None):\n",
    "        #Split into multiple heads\n",
    "        Q_heads =split_heads(X @ W_Q1 , num_heads)\n",
    "        K_heads = split_heads(X@ W_K1, num_heads)\n",
    "        V_heads = split_heads(X@ W_V1, num_heads)\n",
    "        head_outputs = []\n",
    "        head_weights =[]\n",
    "        for i in range(num_heads):\n",
    "            scores = np.matmul(Q_heads[i], K_heads[i].T) / np.sqrt(d_model//num_heads)\n",
    "            if mask is not None:\n",
    "                scores = np.ma.masked_array(scores, mask=mask, fill_value=-1e9)\n",
    "            weights = softmax(scores)\n",
    "            head_output =np.matmul(weights, V_heads[i])\n",
    "            head_outputs.append(head_output)\n",
    "            head_weights.append(weights)\n",
    "        concat = np.transpose(np.array(head_outputs), (1, 0, 2)).reshape(seq_len, d_model)\n",
    "        return concat @ W_O1, head_weights\n",
    "    \n",
    "    W_Q1 = np.random.randn(d_model, d_model)    \n",
    "    W_K1 = np.random.randn(d_model, d_model)\n",
    "    W_V1 = np.random.randn(d_model, d_model)\n",
    "    W_O1 = np.random.randn(d_model, d_model)\n",
    "    \n",
    "    masked_attn_output, masked_attn_weights = masked_multi_head_attention(X_post, mask)\n",
    "    masked_attn_output = layer_norm(X_post + masked_attn_output)\n",
    "    #Subl-layer: encoder-decoder cross attention\n",
    "    def encoder_decoder_attention(decoder_input, encoder_output):\n",
    "        Q_heads = split_heads(decoder_input @ W_Q2, num_heads)\n",
    "        K_heads = split_heads(encoder_output @ W_K2, num_heads)\n",
    "        V_heads = split_heads(encoder_output @ W_V2, num_heads)\n",
    "        head_outputs = []\n",
    "        head_weights = []\n",
    "        for i in range(num_heads):\n",
    "            scores = np.matmul(Q_heads[i], K_heads[i].T) / np.sqrt(d_model//num_heads)\n",
    "            weights = softmax(scores)\n",
    "            head_output = np.matmul(weights, V_heads[i])\n",
    "            head_outputs.append(head_output)\n",
    "            head_weights.append(weights)\n",
    "        concat = np.transpose(np.array(head_outputs), (1, 0, 2)).reshape(seq_len, d_model)\n",
    "        return concat @ W_O2, head_weights\n",
    "    \n",
    "    W_Q2 = np.random.randn(d_model, d_model)    \n",
    "    W_K2 = np.random.randn(d_model, d_model)\n",
    "    W_V2 = np.random.randn(d_model, d_model)\n",
    "    W_O2 = np.random.randn(d_model, d_model)\n",
    "\n",
    "    enc_dec_att_output, enc_dec_attn_weights = encoder_decoder_attention(masked_attn_output, encoder_output)\n",
    "    enc_dec_att_output = layer_norm(masked_attn_output + enc_dec_att_output)\n",
    "\n",
    "    # sublayer: feed forward network\n",
    "    ff_output = feed_forward(enc_dec_att_output)\n",
    "    final_output = layer_norm(enc_dec_att_output + ff_output)\n",
    "    return final_output, {\n",
    "        \"masked_self_attention\": masked_attn_weights,\n",
    "        \"encoder_decoder_attention\": enc_dec_attn_weights\n",
    "    }\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"Creates a causal mask for the decoder to prevent attending to future tokens.\"\n",
    "    mask = np.triu(np.ones((size, size)), k=1).astype(bool)\n",
    "    return mask \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.142194</td>\n",
       "      <td>-2.124573</td>\n",
       "      <td>-0.228127</td>\n",
       "      <td>-0.163755</td>\n",
       "      <td>1.225814</td>\n",
       "      <td>-0.293856</td>\n",
       "      <td>0.471306</td>\n",
       "      <td>1.255386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.506112</td>\n",
       "      <td>-2.087163</td>\n",
       "      <td>0.575351</td>\n",
       "      <td>-0.251184</td>\n",
       "      <td>0.883764</td>\n",
       "      <td>-0.446817</td>\n",
       "      <td>0.507197</td>\n",
       "      <td>1.324964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.482464</td>\n",
       "      <td>-2.081422</td>\n",
       "      <td>0.585979</td>\n",
       "      <td>-0.269717</td>\n",
       "      <td>0.911917</td>\n",
       "      <td>-0.445108</td>\n",
       "      <td>0.441292</td>\n",
       "      <td>1.339524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.445171</td>\n",
       "      <td>-2.161375</td>\n",
       "      <td>0.377629</td>\n",
       "      <td>-0.083675</td>\n",
       "      <td>0.949567</td>\n",
       "      <td>-0.477881</td>\n",
       "      <td>0.641035</td>\n",
       "      <td>1.199870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.142194 -2.124573 -0.228127 -0.163755  1.225814 -0.293856  0.471306   \n",
       "1 -0.506112 -2.087163  0.575351 -0.251184  0.883764 -0.446817  0.507197   \n",
       "2 -0.482464 -2.081422  0.585979 -0.269717  0.911917 -0.445108  0.441292   \n",
       "3 -0.445171 -2.161375  0.377629 -0.083675  0.949567 -0.477881  0.641035   \n",
       "\n",
       "          7  \n",
       "0  1.255386  \n",
       "1  1.324964  \n",
       "2  1.339524  \n",
       "3  1.199870  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len, d_model = 4, 8\n",
    "\n",
    "decoder_input = np.random.rand(seq_len, d_model)\n",
    "econder_output = np.random.rand(seq_len, d_model)\n",
    "mask = create_causal_mask(seq_len)\n",
    "decoder_output, attention_weights = decoder_block(decoder_input, econder_output, num_heads=2, mask=mask)\n",
    "dataframe = pd.DataFrame(decoder_output)\n",
    "dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpunipi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

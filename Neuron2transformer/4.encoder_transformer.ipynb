{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**softmax**\n",
    "\n",
    "- $\\sigma$\t=\tsoftmax\n",
    "- $\\vec{z}$\t=\tinput vector\n",
    "- $e^{z_{i}}$\t=\tstandard exponential function for input vector\n",
    "- $K$\t=\tnumber of classes in the multi-class classifier\n",
    "- $e^{z_{j}}$\t=\tstandard exponential function for output vector\n",
    "- $e^{z_{j}}$\t=\tstandard exponential function for output vector\n",
    "\n",
    "------------\n",
    "![image](/Users/dimtriospanagoulias/Downloads/NLP_UNIPI/nlp_lab/Neuron2transformer/img_blog_image1_inline_(2).webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Proof:\n",
    "\n",
    "- Let's call the max value c\n",
    "    - The modified formula is: $\\frac{e^{x_i-c}}{\\sum_{j=1}^n e^{x_j-c}}$\n",
    "    - This simplifies to: $\\frac{e^{x_i}/e^c}{\\sum_{j=1}^n e^{x_j}/e^c} = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$\n",
    "\n",
    "- - Why Use This Form:\n",
    "\n",
    "    - Prevents numerical overflow\n",
    "    - Avoids inf values when dealing with large numbers\n",
    "    - More stable training in deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# Softmax function for attention mechanism \n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled dot-product attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)  # (seq_len, seq_len)\n",
    "    weights = softmax(scores)\n",
    "    return np.matmul(weights, V), weights  # Output and attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def positional_encoding(seq_len,d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 /np.power(10000, (2*(i//2)) / d_model)\n",
    "    angles = pos * angle_rates\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple layer normalization\n",
    "def layer_norm(X, eps=1e-6):\n",
    "    mean = X.mean( axis=-1, keepdims=True)\n",
    "    std = X.std( axis=-1, keepdims=True)\n",
    "    return (X - mean) / (std + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split heads for multi-head attention\n",
    "def split_heads(X, num_heads):\n",
    "    seq_len , d_model = X.shape\n",
    "    depth = d_model // num_heads\n",
    "    X = X.reshape(seq_len, num_heads, depth)\n",
    "    return np.transpose(X,[1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, num_heads=2):\n",
    "    seq_len, d_model = X.shape\n",
    "    assert d_model % num_heads == 0\n",
    "    W_Q = np.random.randn(d_model, d_model)\n",
    "    W_K = np.random.randn(d_model, d_model)\n",
    "    W_V = np.random.randn(d_model, d_model)\n",
    "    W_O = np.random.randn(d_model, d_model)\n",
    "\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "\n",
    "    Q_heads = split_heads(Q, num_heads)\n",
    "    K_heads = split_heads(K, num_heads)\n",
    "    V_heads = split_heads(V, num_heads)\n",
    "\n",
    "    head_outputs = []\n",
    "    attn_weights = []\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        out, weights = scaled_dot_product_attention(Q_heads[i], K_heads[i], V_heads[i])\n",
    "        head_outputs.append(out)\n",
    "        attn_weights.append(weights)\n",
    "\n",
    "    concat = np.transpose(np.array(head_outputs), (1, 0, 2)).reshape(seq_len, d_model)\n",
    "    output = concat @ W_O\n",
    "\n",
    "    return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X):\n",
    "    W1= np.random.rand( X.shape[1],4)\n",
    "    W2 = np.random.rand(4, X.shape[1])\n",
    "    return np.dot(np.maximum(0,np.dot(X,W1)),W2) # ReLU activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(X, num_heads=2):\n",
    "    # Positional Encoding\n",
    "    pe = positional_encoding(X.shape[0], X.shape[1])\n",
    "    X_pos = X + pe\n",
    "\n",
    "    # Multi-head attention with residual connection and layer norm\n",
    "    attn_output, attn_weights = multi_head_attention(X_pos, num_heads)\n",
    "    attn_output = layer_norm(X_pos + attn_output)\n",
    "\n",
    "    # Feed-forward network with residual and layer norm\n",
    "    ff_output = feed_forward(attn_output)\n",
    "    final_output = layer_norm(attn_output + ff_output)\n",
    "\n",
    "    return final_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input (for 4 words, each with 8 features)\n",
    "np.random.seed(0) # for reproducibility\n",
    "X = np.random.rand(4, 8) # 4 words, each with 8 features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output, attention_weights = encoder_block(X, num_heads=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.322217</td>\n",
       "      <td>-0.703987</td>\n",
       "      <td>-0.625960</td>\n",
       "      <td>-1.116872</td>\n",
       "      <td>0.666319</td>\n",
       "      <td>-0.886659</td>\n",
       "      <td>2.116349</td>\n",
       "      <td>0.228593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.530799</td>\n",
       "      <td>-0.939979</td>\n",
       "      <td>-0.369100</td>\n",
       "      <td>-1.179531</td>\n",
       "      <td>0.499107</td>\n",
       "      <td>-0.845872</td>\n",
       "      <td>2.070712</td>\n",
       "      <td>0.233863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.305734</td>\n",
       "      <td>-0.837191</td>\n",
       "      <td>-0.630886</td>\n",
       "      <td>-1.016384</td>\n",
       "      <td>0.882324</td>\n",
       "      <td>-0.856878</td>\n",
       "      <td>2.062420</td>\n",
       "      <td>0.090862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130583</td>\n",
       "      <td>-0.775043</td>\n",
       "      <td>-0.763410</td>\n",
       "      <td>-0.855956</td>\n",
       "      <td>1.100317</td>\n",
       "      <td>-0.804030</td>\n",
       "      <td>2.050076</td>\n",
       "      <td>-0.082535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.322217 -0.703987 -0.625960 -1.116872  0.666319 -0.886659  2.116349   \n",
       "1  0.530799 -0.939979 -0.369100 -1.179531  0.499107 -0.845872  2.070712   \n",
       "2  0.305734 -0.837191 -0.630886 -1.016384  0.882324 -0.856878  2.062420   \n",
       "3  0.130583 -0.775043 -0.763410 -0.855956  1.100317 -0.804030  2.050076   \n",
       "\n",
       "          7  \n",
       "0  0.228593  \n",
       "1  0.233863  \n",
       "2  0.090862  \n",
       "3 -0.082535  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataframe = pd.DataFrame(encoded_output\n",
    ")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpunipi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
